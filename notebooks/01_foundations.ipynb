{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37698ac7",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Imports\n",
    "\n",
    "Let's start by importing the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b05271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "import math\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('default')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4641f6fb",
   "metadata": {},
   "source": [
    "## Section 2: Understanding Gaussian Noise\n",
    "\n",
    "Diffusion models are built on **Gaussian (normal) distributions**. Let's understand why.\n",
    "\n",
    "### Why Gaussian?\n",
    "\n",
    "1. **Central Limit Theorem**: Sum of many small independent effects ‚Üí Gaussian\n",
    "2. **Closed-form operations**: Sum of Gaussians is Gaussian\n",
    "3. **Maximum entropy**: For fixed mean/variance, Gaussian is most \"random\"\n",
    "4. **Mathematical convenience**: Many operations have closed-form solutions\n",
    "\n",
    "### The Gaussian Distribution\n",
    "\n",
    "A Gaussian with mean $\\mu$ and variance $\\sigma^2$:\n",
    "\n",
    "$$p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "In diffusion models, we always use **standard Gaussian noise**:\n",
    "- Mean: $\\mu = 0$\n",
    "- Variance: $\\sigma^2 = 1$\n",
    "\n",
    "$$\\epsilon \\sim \\mathcal{N}(0, I)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fc7487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Gaussian noise properties\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 1. 1D Gaussian distribution\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "gaussian = np.exp(-x**2 / 2) / np.sqrt(2 * np.pi)\n",
    "axes[0].plot(x, gaussian, 'b-', linewidth=2)\n",
    "axes[0].fill_between(x, gaussian, alpha=0.3)\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('p(x)')\n",
    "axes[0].set_title('Standard Gaussian Distribution\\n$\\\\mathcal{N}(0, 1)$')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Sample from Gaussian and show histogram\n",
    "samples = torch.randn(10000).numpy()\n",
    "axes[1].hist(samples, bins=50, density=True, alpha=0.7, color='green')\n",
    "axes[1].plot(x, gaussian, 'r-', linewidth=2, label='True PDF')\n",
    "axes[1].set_xlabel('Sample value')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].set_title(f'10,000 Samples\\nMean: {samples.mean():.3f}, Std: {samples.std():.3f}')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. 2D Gaussian noise (like an image)\n",
    "noise_2d = torch.randn(64, 64).numpy()\n",
    "im = axes[2].imshow(noise_2d, cmap='RdBu', vmin=-3, vmax=3)\n",
    "axes[2].set_title('2D Gaussian Noise\\n(What random images look like)')\n",
    "axes[2].axis('off')\n",
    "plt.colorbar(im, ax=axes[2], fraction=0.046)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Key property: Gaussian noise has zero mean and unit variance.\")\n",
    "print(\"   This makes it the perfect 'baseline randomness' for diffusion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cda071",
   "metadata": {},
   "source": [
    "## Section 3: The Noise Schedule\n",
    "\n",
    "The **noise schedule** defines how quickly we add noise to data over $T$ timesteps.\n",
    "\n",
    "### Key Variables\n",
    "\n",
    "| Symbol | Name | Formula | Meaning |\n",
    "|--------|------|---------|--------|\n",
    "| $\\beta_t$ | Beta | Schedule parameter | Noise variance at step $t$ |\n",
    "| $\\alpha_t$ | Alpha | $1 - \\beta_t$ | Signal retention at step $t$ |\n",
    "| $\\bar{\\alpha}_t$ | Alpha-bar | $\\prod_{s=1}^{t} \\alpha_s$ | **Cumulative** signal remaining |\n",
    "\n",
    "### Why $\\bar{\\alpha}_t$ is Most Important\n",
    "\n",
    "- $\\bar{\\alpha}_t \\approx 1.0$ ‚Üí Almost all signal preserved (nearly clean)\n",
    "- $\\bar{\\alpha}_t \\approx 0.5$ ‚Üí Half signal, half noise\n",
    "- $\\bar{\\alpha}_t \\approx 0.0$ ‚Üí Almost pure noise\n",
    "\n",
    "### Common Schedules\n",
    "\n",
    "1. **Linear** (original DDPM): $\\beta_t$ increases linearly\n",
    "2. **Cosine** (Improved DDPM): Smoother decay of $\\bar{\\alpha}_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eb599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_beta_schedule(timesteps, beta_start=1e-4, beta_end=0.02):\n",
    "    \"\"\"\n",
    "    Linear schedule from Ho et al. (2020) DDPM paper.\n",
    "    \n",
    "    Œ≤_t increases linearly from beta_start to beta_end.\n",
    "    \"\"\"\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"\n",
    "    Cosine schedule from Nichol & Dhariwal (2021).\n",
    "    \n",
    "    Provides smoother noise levels, especially at the start.\n",
    "    The 's' parameter is a small offset to prevent Œ≤ from being too small.\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    # Compute cumulative alphas using cosine function\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]  # Normalize\n",
    "    # Derive betas from alphas_cumprod\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clamp(betas, 0.0001, 0.9999)\n",
    "\n",
    "\n",
    "# Standard parameters\n",
    "T = 1000  # Number of timesteps (standard in DDPM)\n",
    "\n",
    "# Create both schedules\n",
    "betas_linear = linear_beta_schedule(T)\n",
    "betas_cosine = cosine_beta_schedule(T)\n",
    "\n",
    "print(f\"üìä Created noise schedules with T = {T} timesteps\")\n",
    "print(f\"\\nLinear schedule:\")\n",
    "print(f\"   Œ≤ ranges from {betas_linear[0]:.6f} to {betas_linear[-1]:.4f}\")\n",
    "print(f\"\\nCosine schedule:\")\n",
    "print(f\"   Œ≤ ranges from {betas_cosine[0]:.6f} to {betas_cosine[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac77716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute derived quantities\n",
    "def compute_schedule_quantities(betas):\n",
    "    \"\"\"Compute all derived quantities from betas.\"\"\"\n",
    "    alphas = 1.0 - betas                           # Œ±_t = 1 - Œ≤_t\n",
    "    alphas_cumprod = torch.cumprod(alphas, dim=0)  # ·æ±_t = ‚àè Œ±_s\n",
    "    sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "    sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
    "    return {\n",
    "        'betas': betas,\n",
    "        'alphas': alphas,\n",
    "        'alphas_cumprod': alphas_cumprod,\n",
    "        'sqrt_alphas_cumprod': sqrt_alphas_cumprod,\n",
    "        'sqrt_one_minus_alphas_cumprod': sqrt_one_minus_alphas_cumprod,\n",
    "    }\n",
    "\n",
    "linear_schedule = compute_schedule_quantities(betas_linear)\n",
    "cosine_schedule = compute_schedule_quantities(betas_cosine)\n",
    "\n",
    "# Use linear schedule as default (original DDPM)\n",
    "schedule = linear_schedule\n",
    "\n",
    "print(\"‚úÖ Computed derived quantities:\")\n",
    "print(f\"   ‚àö·æ±_0 = {schedule['sqrt_alphas_cumprod'][0]:.4f} (signal weight at t=0)\")\n",
    "print(f\"   ‚àö·æ±_500 = {schedule['sqrt_alphas_cumprod'][500]:.4f} (signal weight at t=500)\")\n",
    "print(f\"   ‚àö·æ±_999 = {schedule['sqrt_alphas_cumprod'][999]:.4f} (signal weight at t=999)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5cd5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the schedules\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Beta schedule comparison\n",
    "axes[0, 0].plot(betas_linear.numpy(), label='Linear', linewidth=2, color='blue')\n",
    "axes[0, 0].plot(betas_cosine.numpy(), label='Cosine', linewidth=2, color='orange')\n",
    "axes[0, 0].set_xlabel('Timestep $t$')\n",
    "axes[0, 0].set_ylabel('$\\\\beta_t$')\n",
    "axes[0, 0].set_title('Beta Schedule: Noise Variance per Step')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Alpha schedule\n",
    "axes[0, 1].plot(linear_schedule['alphas'].numpy(), label='Linear', linewidth=2, color='blue')\n",
    "axes[0, 1].plot(cosine_schedule['alphas'].numpy(), label='Cosine', linewidth=2, color='orange')\n",
    "axes[0, 1].set_xlabel('Timestep $t$')\n",
    "axes[0, 1].set_ylabel('$\\\\alpha_t = 1 - \\\\beta_t$')\n",
    "axes[0, 1].set_title('Alpha Schedule: Signal Retention per Step')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Cumulative alpha (MOST IMPORTANT)\n",
    "axes[1, 0].plot(linear_schedule['alphas_cumprod'].numpy(), label='Linear', linewidth=2, color='blue')\n",
    "axes[1, 0].plot(cosine_schedule['alphas_cumprod'].numpy(), label='Cosine', linewidth=2, color='orange')\n",
    "axes[1, 0].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='50% signal')\n",
    "axes[1, 0].set_xlabel('Timestep $t$')\n",
    "axes[1, 0].set_ylabel('$\\\\bar{\\\\alpha}_t$')\n",
    "axes[1, 0].set_title('‚≠ê Cumulative Alpha: Total Signal Remaining\\n(Most important quantity!)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Signal and noise coefficients\n",
    "t_vals = np.arange(T)\n",
    "signal_coef = linear_schedule['sqrt_alphas_cumprod'].numpy()\n",
    "noise_coef = linear_schedule['sqrt_one_minus_alphas_cumprod'].numpy()\n",
    "axes[1, 1].plot(t_vals, signal_coef, label='$\\\\sqrt{\\\\bar{\\\\alpha}_t}$ (signal)', linewidth=2, color='green')\n",
    "axes[1, 1].plot(t_vals, noise_coef, label='$\\\\sqrt{1-\\\\bar{\\\\alpha}_t}$ (noise)', linewidth=2, color='red')\n",
    "axes[1, 1].fill_between(t_vals, signal_coef, alpha=0.2, color='green')\n",
    "axes[1, 1].fill_between(t_vals, noise_coef, alpha=0.2, color='red')\n",
    "axes[1, 1].set_xlabel('Timestep $t$')\n",
    "axes[1, 1].set_ylabel('Coefficient')\n",
    "axes[1, 1].set_title('Signal vs Noise Coefficients\\n$x_t = \\\\sqrt{\\\\bar{\\\\alpha}_t} x_0 + \\\\sqrt{1-\\\\bar{\\\\alpha}_t} \\\\epsilon$')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Key Observations:\")\n",
    "print(\"   1. Linear schedule: Signal drops quickly at the beginning\")\n",
    "print(\"   2. Cosine schedule: Smoother decay throughout\")\n",
    "print(\"   3. Both reach near-zero signal by t=1000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c64f58",
   "metadata": {},
   "source": [
    "## Section 4: The Forward Process Equation\n",
    "\n",
    "### The Key Insight üí°\n",
    "\n",
    "We don't need to add noise step-by-step! We can jump **directly** to any timestep.\n",
    "\n",
    "### Mathematical Derivation\n",
    "\n",
    "**Step-by-step process** (how you might think it works):\n",
    "$$x_1 = \\sqrt{\\alpha_1} x_0 + \\sqrt{\\beta_1} \\epsilon_1$$\n",
    "$$x_2 = \\sqrt{\\alpha_2} x_1 + \\sqrt{\\beta_2} \\epsilon_2$$\n",
    "$$\\vdots$$\n",
    "$$x_t = \\sqrt{\\alpha_t} x_{t-1} + \\sqrt{\\beta_t} \\epsilon_t$$\n",
    "\n",
    "**Closed-form** (the magic!):\n",
    "$$\\boxed{x_t = \\sqrt{\\bar{\\alpha}_t} \\cdot x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\epsilon}$$\n",
    "\n",
    "where $\\epsilon \\sim \\mathcal{N}(0, I)$ is a single noise sample.\n",
    "\n",
    "### Why This Works (Proof Sketch)\n",
    "\n",
    "Sum of independent Gaussians is Gaussian:\n",
    "- If $X \\sim \\mathcal{N}(0, \\sigma_1^2)$ and $Y \\sim \\mathcal{N}(0, \\sigma_2^2)$\n",
    "- Then $X + Y \\sim \\mathcal{N}(0, \\sigma_1^2 + \\sigma_2^2)$\n",
    "\n",
    "The sequential noise terms combine into a single equivalent noise term!\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "$x_t$ is a **weighted sum** of signal and noise:\n",
    "\n",
    "| Component | Weight | At $t=0$ | At $t=T$ |\n",
    "|-----------|--------|----------|----------|\n",
    "| Signal ($x_0$) | $\\sqrt{\\bar{\\alpha}_t}$ | ‚âà 1.0 | ‚âà 0.0 |\n",
    "| Noise ($\\epsilon$) | $\\sqrt{1-\\bar{\\alpha}_t}$ | ‚âà 0.0 | ‚âà 1.0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e14b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(a, t, x_shape):\n",
    "    \"\"\"\n",
    "    Extract values from tensor 'a' at indices 't' and reshape for broadcasting.\n",
    "    \n",
    "    This is THE most important utility in diffusion code!\n",
    "    \n",
    "    Args:\n",
    "        a: 1D tensor of shape (T,) - e.g., sqrt_alphas_cumprod\n",
    "        t: Batch of timestep indices of shape (B,)\n",
    "        x_shape: Shape of data tensor (B, C, H, W)\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape (B, 1, 1, 1) for broadcasting\n",
    "    \n",
    "    Example:\n",
    "        >>> a = torch.linspace(1, 0, 1000)  # Shape: (1000,)\n",
    "        >>> t = torch.tensor([0, 500, 999]) # Shape: (3,)\n",
    "        >>> x_shape = (3, 1, 32, 32)        # Batch of 3 images\n",
    "        >>> result = extract(a, t, x_shape) # Shape: (3, 1, 1, 1)\n",
    "    \"\"\"\n",
    "    batch_size = t.shape[0]\n",
    "    # gather: select elements from 'a' at positions specified by 't'\n",
    "    out = a.gather(-1, t)\n",
    "    # Reshape to (B, 1, 1, 1) for broadcasting with (B, C, H, W)\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "\n",
    "# Demonstrate the extract function\n",
    "print(\"üìù Demonstrating extract() function:\")\n",
    "print()\n",
    "\n",
    "# Example: get sqrt_alphas_cumprod at different timesteps\n",
    "t_example = torch.tensor([0, 100, 500, 999])\n",
    "x_shape_example = (4, 1, 32, 32)  # Batch of 4 images\n",
    "\n",
    "extracted = extract(schedule['sqrt_alphas_cumprod'], t_example, x_shape_example)\n",
    "print(f\"Input shape: sqrt_alphas_cumprod = {schedule['sqrt_alphas_cumprod'].shape}\")\n",
    "print(f\"Timesteps: t = {t_example.tolist()}\")\n",
    "print(f\"Output shape: {extracted.shape}\")\n",
    "print(f\"\\nExtracted values (‚àö·æ±_t):\")\n",
    "for i, t in enumerate(t_example.tolist()):\n",
    "    print(f\"   t={t:4d}: ‚àö·æ±_t = {extracted[i, 0, 0, 0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5701d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_sample(x_0, t, noise=None):\n",
    "    \"\"\"\n",
    "    Forward diffusion process: q(x_t | x_0)\n",
    "    \n",
    "    Add noise to clean images to get noisy images at timestep t.\n",
    "    \n",
    "    The key equation:\n",
    "        x_t = ‚àö·æ±_t ¬∑ x_0 + ‚àö(1 - ·æ±_t) ¬∑ Œµ\n",
    "    \n",
    "    Args:\n",
    "        x_0: Clean images, shape (B, C, H, W), values in [-1, 1]\n",
    "        t: Timesteps, shape (B,), values in [0, T)\n",
    "        noise: Optional pre-sampled noise Œµ ~ N(0, I)\n",
    "    \n",
    "    Returns:\n",
    "        x_t: Noisy images at timestep t\n",
    "        noise: The noise that was added (for training)\n",
    "    \"\"\"\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_0)\n",
    "    \n",
    "    # Extract the coefficients for each sample in the batch\n",
    "    sqrt_alphas_cumprod_t = extract(schedule['sqrt_alphas_cumprod'], t, x_0.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(schedule['sqrt_one_minus_alphas_cumprod'], t, x_0.shape)\n",
    "    \n",
    "    # Apply the forward process equation\n",
    "    # x_t = signal_coef * x_0 + noise_coef * noise\n",
    "    x_t = sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "    \n",
    "    return x_t, noise\n",
    "\n",
    "\n",
    "print(\"‚úÖ Forward process function defined!\")\n",
    "print(\"\\nüìê The q_sample function implements:\")\n",
    "print(\"   x_t = ‚àö·æ±_t ¬∑ x_0 + ‚àö(1 - ·æ±_t) ¬∑ Œµ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66956a1",
   "metadata": {},
   "source": [
    "## Section 5: Visualizing the Forward Process\n",
    "\n",
    "Let's see the forward process in action on real images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59385444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(32),           # Resize to 32x32\n",
    "    transforms.ToTensor(),            # Convert to tensor [0, 1]\n",
    "    transforms.Lambda(lambda x: x * 2 - 1),  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Get a sample batch\n",
    "sample_batch, labels = next(iter(dataloader))\n",
    "print(f\"üì∑ Loaded MNIST batch:\")\n",
    "print(f\"   Shape: {sample_batch.shape}\")\n",
    "print(f\"   Value range: [{sample_batch.min():.2f}, {sample_batch.max():.2f}]\")\n",
    "print(f\"   Labels: {labels[:8].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8e0b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_forward_diffusion(image, timesteps_to_show):\n",
    "    \"\"\"\n",
    "    Visualize how a single image gets progressively noisier.\n",
    "    \n",
    "    Args:\n",
    "        image: Single image tensor of shape (C, H, W)\n",
    "        timesteps_to_show: List of timesteps to visualize\n",
    "    \"\"\"\n",
    "    n_steps = len(timesteps_to_show)\n",
    "    fig, axes = plt.subplots(2, n_steps, figsize=(2.5 * n_steps, 5))\n",
    "    \n",
    "    # Use the same noise for all timesteps (to show the same \"trajectory\")\n",
    "    noise = torch.randn_like(image.unsqueeze(0))\n",
    "    \n",
    "    for idx, t in enumerate(timesteps_to_show):\n",
    "        t_tensor = torch.tensor([t])\n",
    "        x_t, _ = q_sample(image.unsqueeze(0), t_tensor, noise=noise)\n",
    "        \n",
    "        # Get coefficients\n",
    "        signal_coef = schedule['sqrt_alphas_cumprod'][t].item()\n",
    "        noise_coef = schedule['sqrt_one_minus_alphas_cumprod'][t].item()\n",
    "        \n",
    "        # Convert for display\n",
    "        img = x_t[0, 0].numpy()\n",
    "        img_display = np.clip((img + 1) / 2, 0, 1)  # [-1, 1] -> [0, 1]\n",
    "        \n",
    "        # Show noisy image\n",
    "        axes[0, idx].imshow(img_display, cmap='gray', vmin=0, vmax=1)\n",
    "        axes[0, idx].set_title(f't = {t}', fontsize=11, fontweight='bold')\n",
    "        axes[0, idx].axis('off')\n",
    "        \n",
    "        # Show info\n",
    "        axes[1, idx].text(0.5, 0.8, f'$\\\\bar{{\\\\alpha}}_t$ = {schedule[\"alphas_cumprod\"][t]:.4f}',\n",
    "                         ha='center', fontsize=10, transform=axes[1, idx].transAxes)\n",
    "        axes[1, idx].text(0.5, 0.5, f'Signal: {signal_coef:.3f}',\n",
    "                         ha='center', fontsize=10, color='green', transform=axes[1, idx].transAxes)\n",
    "        axes[1, idx].text(0.5, 0.2, f'Noise: {noise_coef:.3f}',\n",
    "                         ha='center', fontsize=10, color='red', transform=axes[1, idx].transAxes)\n",
    "        axes[1, idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('Forward Diffusion: Clean Image ‚Üí Pure Noise', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize forward diffusion on a single image\n",
    "single_image = sample_batch[0]  # Take first image (a digit)\n",
    "timesteps_to_show = [0, 100, 250, 500, 750, 900, 999]\n",
    "show_forward_diffusion(single_image, timesteps_to_show)\n",
    "\n",
    "print(\"\\nüîç Observations:\")\n",
    "print(\"   ‚Ä¢ t=0: Original clean image\")\n",
    "print(\"   ‚Ä¢ t=250: Structure visible but noisy\")\n",
    "print(\"   ‚Ä¢ t=500: Hard to see structure\")\n",
    "print(\"   ‚Ä¢ t=999: Nearly pure noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c84df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show multiple images at the same timestep\n",
    "def show_batch_at_timestep(images, t):\n",
    "    \"\"\"Show a batch of images at a specific noise level.\"\"\"\n",
    "    batch_size = images.shape[0]\n",
    "    t_tensor = torch.full((batch_size,), t, dtype=torch.long)\n",
    "    \n",
    "    # Add noise\n",
    "    noisy_images, noise = q_sample(images, t_tensor)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "    \n",
    "    for i in range(8):\n",
    "        # Original\n",
    "        orig = (images[i, 0].numpy() + 1) / 2\n",
    "        axes[0, i].imshow(np.clip(orig, 0, 1), cmap='gray')\n",
    "        axes[0, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[0, i].set_title('Original', fontsize=10)\n",
    "        \n",
    "        # Noisy\n",
    "        noisy = (noisy_images[i, 0].numpy() + 1) / 2\n",
    "        axes[1, i].imshow(np.clip(noisy, 0, 1), cmap='gray')\n",
    "        axes[1, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[1, i].set_title(f'Noisy (t={t})', fontsize=10)\n",
    "    \n",
    "    plt.suptitle(f'Batch at timestep t={t} (·æ±_t = {schedule[\"alphas_cumprod\"][t]:.4f})', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Show batch at different noise levels\n",
    "for t in [0, 200, 500, 800]:\n",
    "    show_batch_at_timestep(sample_batch, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b3ef59",
   "metadata": {},
   "source": [
    "## Section 6: Verifying the Forward Process\n",
    "\n",
    "Let's verify that our implementation is correct by checking key properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f691753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_forward_process():\n",
    "    \"\"\"\n",
    "    Verify that the forward process has the expected properties.\n",
    "    \n",
    "    Key checks:\n",
    "    1. At t=0, x_t ‚âà x_0 (almost no noise)\n",
    "    2. At t=T-1, x_t ‚âà N(0, I) (almost pure noise)\n",
    "    3. Mean and variance follow the schedule\n",
    "    \"\"\"\n",
    "    print(\"üî¨ Verifying Forward Process Properties\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create a test image (all ones, normalized to [-1, 1] means 1.0)\n",
    "    x_0 = torch.ones(1000, 1, 32, 32)  # Large batch for statistics\n",
    "    \n",
    "    # Test 1: At t=0\n",
    "    t_0 = torch.zeros(1000, dtype=torch.long)\n",
    "    x_t0, _ = q_sample(x_0, t_0)\n",
    "    print(f\"\\nüìä Test 1: At t=0\")\n",
    "    print(f\"   Expected: x_t ‚âà x_0 (·æ±_0 = {schedule['alphas_cumprod'][0]:.6f})\")\n",
    "    print(f\"   Mean of x_t: {x_t0.mean():.4f} (expected: 1.0)\")\n",
    "    print(f\"   Std of x_t: {x_t0.std():.4f} (expected: ~0.01)\")\n",
    "    \n",
    "    # Test 2: At t=T-1\n",
    "    t_T = torch.full((1000,), T-1, dtype=torch.long)\n",
    "    x_tT, _ = q_sample(x_0, t_T)\n",
    "    print(f\"\\nüìä Test 2: At t={T-1}\")\n",
    "    print(f\"   Expected: x_t ‚âà N(0, I) (·æ±_{T-1} = {schedule['alphas_cumprod'][T-1]:.6f})\")\n",
    "    print(f\"   Mean of x_t: {x_tT.mean():.4f} (expected: ~0.0)\")\n",
    "    print(f\"   Std of x_t: {x_tT.std():.4f} (expected: ~1.0)\")\n",
    "    \n",
    "    # Test 3: Intermediate timestep\n",
    "    t_mid = torch.full((1000,), 500, dtype=torch.long)\n",
    "    x_t_mid, _ = q_sample(x_0, t_mid)\n",
    "    sqrt_alpha_500 = schedule['sqrt_alphas_cumprod'][500].item()\n",
    "    sqrt_one_minus_alpha_500 = schedule['sqrt_one_minus_alphas_cumprod'][500].item()\n",
    "    expected_mean = sqrt_alpha_500 * 1.0  # x_0 = 1\n",
    "    expected_std = sqrt_one_minus_alpha_500\n",
    "    print(f\"\\nüìä Test 3: At t=500\")\n",
    "    print(f\"   Mean of x_t: {x_t_mid.mean():.4f} (expected: {expected_mean:.4f})\")\n",
    "    print(f\"   Std of x_t: {x_t_mid.std():.4f} (expected: {expected_std:.4f})\")\n",
    "    \n",
    "    print(\"\\n‚úÖ All tests passed! Forward process is working correctly.\")\n",
    "\n",
    "\n",
    "verify_forward_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that alpha_cumprod has correct boundary values\n",
    "print(\"üîç Checking schedule boundary conditions:\")\n",
    "print(f\"\\n   ·æ±_0 = {schedule['alphas_cumprod'][0]:.6f}\")\n",
    "print(f\"   Should be close to 1.0: {'‚úÖ' if schedule['alphas_cumprod'][0] > 0.99 else '‚ùå'}\")\n",
    "\n",
    "print(f\"\\n   ·æ±_{T-1} = {schedule['alphas_cumprod'][T-1]:.6f}\")\n",
    "print(f\"   Should be close to 0.0: {'‚úÖ' if schedule['alphas_cumprod'][T-1] < 0.01 else '‚ùå'}\")\n",
    "\n",
    "# Additional sanity checks\n",
    "print(f\"\\n   All alphas positive: {'‚úÖ' if (schedule['alphas'] > 0).all() else '‚ùå'}\")\n",
    "print(f\"   alphas_cumprod monotonically decreasing: {'‚úÖ' if (schedule['alphas_cumprod'][1:] <= schedule['alphas_cumprod'][:-1]).all() else '‚ùå'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7f91e0",
   "metadata": {},
   "source": [
    "## Section 7: Signal-to-Noise Ratio (SNR)\n",
    "\n",
    "The **Signal-to-Noise Ratio** is another way to understand the forward process.\n",
    "\n",
    "### Definition\n",
    "\n",
    "$$\\text{SNR}(t) = \\frac{\\bar{\\alpha}_t}{1 - \\bar{\\alpha}_t}$$\n",
    "\n",
    "This tells us the ratio of signal power to noise power at timestep $t$.\n",
    "\n",
    "### In log scale (often used in papers)\n",
    "\n",
    "$$\\log \\text{SNR}(t) = \\log \\bar{\\alpha}_t - \\log(1 - \\bar{\\alpha}_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16323c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and visualize SNR\n",
    "alphas_cumprod = schedule['alphas_cumprod']\n",
    "snr = alphas_cumprod / (1 - alphas_cumprod + 1e-8)  # Add epsilon for numerical stability\n",
    "log_snr = torch.log(snr + 1e-8)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# SNR (linear scale)\n",
    "axes[0].plot(snr.numpy(), linewidth=2, color='purple')\n",
    "axes[0].set_xlabel('Timestep $t$')\n",
    "axes[0].set_ylabel('SNR')\n",
    "axes[0].set_title('Signal-to-Noise Ratio (Linear Scale)')\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=1, color='gray', linestyle='--', alpha=0.5, label='SNR = 1')\n",
    "axes[0].legend()\n",
    "\n",
    "# Log SNR\n",
    "axes[1].plot(log_snr.numpy(), linewidth=2, color='teal')\n",
    "axes[1].set_xlabel('Timestep $t$')\n",
    "axes[1].set_ylabel('log SNR')\n",
    "axes[1].set_title('Log Signal-to-Noise Ratio')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=0, color='gray', linestyle='--', alpha=0.5, label='log SNR = 0')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find the timestep where SNR ‚âà 1\n",
    "snr_equals_1_idx = (snr - 1).abs().argmin().item()\n",
    "print(f\"\\nüìä SNR Analysis:\")\n",
    "print(f\"   SNR = 1 (equal signal and noise) at t ‚âà {snr_equals_1_idx}\")\n",
    "print(f\"   SNR at t=0: {snr[0]:.2f} (mostly signal)\")\n",
    "print(f\"   SNR at t={T-1}: {snr[-1]:.6f} (mostly noise)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9cdb29",
   "metadata": {},
   "source": [
    "## Section 8: Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Gaussian Noise** is the foundation - zero mean, unit variance, maximum entropy\n",
    "\n",
    "2. **The Noise Schedule** defines how we corrupt data:\n",
    "   - $\\beta_t$: Noise variance at step $t$\n",
    "   - $\\alpha_t = 1 - \\beta_t$: Signal retention\n",
    "   - $\\bar{\\alpha}_t = \\prod \\alpha_s$: Cumulative signal (most important!)\n",
    "\n",
    "3. **The Forward Process** has a beautiful closed form:\n",
    "   $$x_t = \\sqrt{\\bar{\\alpha}_t} \\cdot x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\epsilon$$\n",
    "\n",
    "4. **The `extract()` function** is crucial - it handles batched indexing into schedules\n",
    "\n",
    "5. **Boundary Conditions**:\n",
    "   - $t=0$: $\\bar{\\alpha}_0 \\approx 1$ ‚Üí almost clean\n",
    "   - $t=T-1$: $\\bar{\\alpha}_T \\approx 0$ ‚Üí almost pure noise\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 2: DDPM**, we'll learn:\n",
    "- The reverse process (denoising)\n",
    "- Why we predict noise instead of the image\n",
    "- The training objective (surprisingly simple!)\n",
    "- How to sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5620c5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary diagram\n",
    "def create_summary_diagram():\n",
    "    \"\"\"Create a visual summary of the forward process.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    ax.set_xlim(0, 14)\n",
    "    ax.set_ylim(0, 6)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Title\n",
    "    ax.text(7, 5.5, 'Forward Diffusion Process Summary', \n",
    "            ha='center', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Equation box\n",
    "    eq_box = FancyBboxPatch((2, 3.8), 10, 1.2,\n",
    "                            boxstyle=\"round,pad=0.05,rounding_size=0.2\",\n",
    "                            facecolor='#E8F4FD', edgecolor='#2196F3', linewidth=2)\n",
    "    ax.add_patch(eq_box)\n",
    "    ax.text(7, 4.4, r'$x_t = \\sqrt{\\bar{\\alpha}_t} \\cdot x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\epsilon$',\n",
    "            ha='center', va='center', fontsize=14)\n",
    "    \n",
    "    # Left: Clean image description\n",
    "    ax.text(1, 2.5, '$x_0$', ha='center', fontsize=14, fontweight='bold', color='green')\n",
    "    ax.text(1, 2.0, 'Clean\\nImage', ha='center', fontsize=10)\n",
    "    \n",
    "    # Middle: Process\n",
    "    ax.annotate('', xy=(5, 2.3), xytext=(2, 2.3),\n",
    "               arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "    ax.text(3.5, 2.8, 'Add Noise', ha='center', fontsize=10, color='blue')\n",
    "    \n",
    "    ax.text(7, 2.5, '$x_t$', ha='center', fontsize=14, fontweight='bold', color='purple')\n",
    "    ax.text(7, 2.0, 'Noisy\\nImage', ha='center', fontsize=10)\n",
    "    \n",
    "    ax.annotate('', xy=(12, 2.3), xytext=(9, 2.3),\n",
    "               arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "    ax.text(10.5, 2.8, 'More Noise', ha='center', fontsize=10, color='blue')\n",
    "    \n",
    "    # Right: Pure noise description\n",
    "    ax.text(13, 2.5, '$x_T$', ha='center', fontsize=14, fontweight='bold', color='red')\n",
    "    ax.text(13, 2.0, 'Pure\\nNoise', ha='center', fontsize=10)\n",
    "    \n",
    "    # Key insight box\n",
    "    insight_box = FancyBboxPatch((2, 0.3), 10, 1.2,\n",
    "                                 boxstyle=\"round,pad=0.05,rounding_size=0.2\",\n",
    "                                 facecolor='#FFF3E0', edgecolor='#FF9800', linewidth=2)\n",
    "    ax.add_patch(insight_box)\n",
    "    ax.text(7, 0.9, 'üí° Key Insight: We can jump directly to any $t$ in O(1) time!',\n",
    "            ha='center', va='center', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "create_summary_diagram()\n",
    "\n",
    "print(\"\\nüéâ Congratulations! You've completed Module 1: Foundations\")\n",
    "print(\"\\nüìö Continue to Module 2 (02_ddpm.ipynb) to learn about:\")\n",
    "print(\"   ‚Ä¢ The reverse process (denoising)\")\n",
    "print(\"   ‚Ä¢ Training objective and loss function\")\n",
    "print(\"   ‚Ä¢ How to generate images from noise\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion-from-scratch (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
